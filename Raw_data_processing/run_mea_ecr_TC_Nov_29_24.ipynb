{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze signals from each organoid, to determine viability and spike thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from pprint import pprint\n",
    "import glob\n",
    "\n",
    "from oitools import ecr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['April 19 2024',\n",
       " '2024May28 No window data ',\n",
       " 'March 30 2024',\n",
       " 'Nov_18_2024',\n",
       " '3_org_stim_vs_1_organoid',\n",
       " 'Nov_18_2024_ecr_results3_no_window',\n",
       " 'Dec_10_2024',\n",
       " 'Dec_10_2024_ecr_results_no_window']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = \"/cis/project/organoid\"\n",
    "os.listdir(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['misc',\n",
       " 'Overview Document Run 28 Stimulation Experiment 3 Organoids vs 1 Organoid.docx',\n",
       " '241018']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(root, os.listdir(root)[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: April 19 2024, Size: 12.03 GB\n",
      "Directory: 2024May28 No window data , Size: 13.31 GB\n",
      "Directory: March 30 2024, Size: 18.59 GB\n",
      "Directory: Nov_18_2024, Size: 10.69 GB\n",
      "Directory: 3_org_stim_vs_1_organoid, Size: 10.38 GB\n",
      "Directory: Nov_18_2024_ecr_results3_no_window, Size: 0.08 GB\n",
      "Directory: Dec_10_2024, Size: 58.30 GB\n",
      "Directory: Dec_10_2024_ecr_results_no_window, Size: 1.07 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_folder_size_in_mb(folder_path):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            total_size += os.path.getsize(file_path)\n",
    "    size_mb = total_size / (1024 * 1024*1024)\n",
    "    return size_mb\n",
    "\n",
    "root = \"/cis/project/organoid\"\n",
    "\n",
    "# List all items in the root directory\n",
    "items = os.listdir(root)\n",
    "\n",
    "# Iterate over the items and check if they are directories\n",
    "for item in items:\n",
    "    item_path = os.path.join(root, item)\n",
    "    if os.path.isdir(item_path):\n",
    "        folder_size_mb = get_folder_size_in_mb(item_path)\n",
    "        print(f\"Directory: {item}, Size: {folder_size_mb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data.raw_20241213_18h15m.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"/cis/project/organoid/Dec_10_2024_ecr_results_no_window/241018/M07915/Stimulation/000295\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cis/project/organoid/Nov_18_2024\n"
     ]
    }
   ],
   "source": [
    "# Path of source .h5 files\n",
    "root = \"/cis/project/organoid\"\n",
    "path_source_files = os.path.join(root, os.listdir(root)[3])\n",
    "print(path_source_files)\n",
    "\n",
    "# Path where results will be stored\n",
    "path_results = '/cis/project/organoid/Dec_10_2024_ecr_results_no_window'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a list of files that we'll analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exluding files of size less than 2764.922 MB.\n",
      "\n",
      "Assessing these h5 files in /cis/project/organoid/Nov_18_2024.\n",
      "['data.raw.h5']\n"
     ]
    }
   ],
   "source": [
    "# Get a list of files that we'll analyze\n",
    "\n",
    "# Get list of all h5 files in the target directory\n",
    "filenames = os.listdir(path_source_files)\n",
    "filenames = [fn for fn in filenames if fn.endswith('.h5')]\n",
    "\n",
    "# Get list of file sizes\n",
    "sizes = []\n",
    "for f in filenames:\n",
    "    sizes.append(os.stat(os.path.join(path_source_files, f)).st_size)\n",
    "\n",
    "# Excluding files that are relatively small\n",
    "thresh = np.median(sizes)/4\n",
    "print(f'Exluding files of size less than {thresh/1e6:0.3f} MB.')\n",
    "filenames = [fn for fn, sz in zip(filenames, sizes) if sz>=thresh]\n",
    "\n",
    "print(f'\\nAssessing these h5 files in {path_source_files}.')\n",
    "pprint(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out well names for each h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data.raw.h5:\n",
      "\t['well000', 'well001', 'well002', 'well003', 'well004', 'well005']\n"
     ]
    }
   ],
   "source": [
    "# Print out well names for each file\n",
    "\n",
    "for filename in filenames:\n",
    "    fullname = os.path.join(path_source_files, filename)\n",
    "    ephys_file = h5py.File(fullname, 'r')\n",
    "    \n",
    "    print(f'\\n{filename}:\\n\\t', end='')\n",
    "    print(list(ephys_file['recordings']['rec0000'].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze data well by well (organoid by organoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1 of 1, data.raw.h5...\n",
      "Processing well 1 of 6, well000...\n",
      "5% spike amplitude percentile is -22.629671096801758.\n",
      "Recording duration is 600.0772 seconds.\n",
      "Channel spike count threshold is 5.949471816287769\n",
      "Number of channels kept is 685.\n",
      "\n",
      "Using maximum of 15 CPUs for processing.\n",
      "\n",
      "Done. Processing took 53.76 seconds.\n",
      "\n",
      "\n",
      "Processing well 2 of 6, well001...\n",
      "5% spike amplitude percentile is -15.84598445892334.\n",
      "Recording duration is 600.0689 seconds.\n",
      "Channel spike count threshold is 5.842258973142006\n",
      "Number of channels kept is 777.\n",
      "\n",
      "Using maximum of 15 CPUs for processing.\n",
      "\n",
      "Done. Processing took 63.08 seconds.\n",
      "\n",
      "\n",
      "Processing well 3 of 6, well002...\n",
      "5% spike amplitude percentile is -20.35711097717285.\n",
      "Recording duration is 600.0415 seconds.\n",
      "Channel spike count threshold is 6.512066747803693\n",
      "Number of channels kept is 1006.\n",
      "\n",
      "Using maximum of 15 CPUs for processing.\n",
      "\n",
      "Done. Processing took 368.01 seconds.\n",
      "\n",
      "\n",
      "Processing well 4 of 6, well003...\n",
      "5% spike amplitude percentile is -13.032979965209961.\n",
      "Recording duration is 600.0890 seconds.\n",
      "Channel spike count threshold is 6.5058407571425665\n",
      "Number of channels kept is 999.\n",
      "\n",
      "Using maximum of 15 CPUs for processing.\n",
      "\n",
      "Done. Processing took 169.66 seconds.\n",
      "\n",
      "\n",
      "Processing well 5 of 6, well004...\n",
      "5% spike amplitude percentile is -14.003625869750977.\n",
      "Recording duration is 600.0457 seconds.\n",
      "Channel spike count threshold is 6.595320613586895\n",
      "Number of channels kept is 858.\n",
      "\n",
      "Using maximum of 15 CPUs for processing.\n",
      "\n",
      "Done. Processing took 113.12 seconds.\n",
      "\n",
      "\n",
      "Processing well 6 of 6, well005...\n",
      "5% spike amplitude percentile is -18.0568790435791.\n",
      "Recording duration is 599.4907 seconds.\n",
      "Channel spike count threshold is 5.675355143650394\n",
      "Number of channels kept is 565.\n",
      "\n",
      "Using maximum of 15 CPUs for processing.\n",
      "\n",
      "Done. Processing took 36.72 seconds.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs = 10000  # Sampling rate for all Maxwell spike data\n",
    "\n",
    "## Values used to filter out invidual spikes, and channel with low spike count\n",
    "thresh_spike_amp_percentile = 5  # Discard the low P percentile of spikes, which may just be noise\n",
    "thresh_spikes_per_chan_std = 2  # Discard channels with < mean(n_spikes)+spike_cnt_thresh_std*std(n_spikes), on log scale\n",
    "\n",
    "## Values used for Puppa's ECR algorithm\n",
    "amp_thresh_percentile = None\n",
    "amp_thresh_std = 1\n",
    "adj_threshold = 1.0\n",
    "time_resolution = 1/fs\n",
    "# Values used in Puppa paper, on real spike data...\n",
    "raster_dur = 0.0005\n",
    "corr_type = 'cc'  ## 'cc'==cross-correlation, 'pearson' not yet implemented\n",
    "epsilon = 0.003\n",
    "T_list = np.array([0.020, 0.0175, 0.016])\n",
    "sigma_list = np.array([0.0004, 0.00055, 0.0007])\n",
    "\n",
    "parameters = {}\n",
    "parameters['epsilon'] = epsilon\n",
    "parameters['T_list'] = T_list\n",
    "parameters['sigma_list'] = sigma_list\n",
    "parameters['amp_thresh_percentile'] = amp_thresh_percentile\n",
    "parameters['amp_thresh_std'] = amp_thresh_std\n",
    "parameters['raster_dur'] = raster_dur\n",
    "parameters['corr_type'] = corr_type\n",
    "parameters['adj_threshold'] = adj_threshold\n",
    "parameters['time_resolution'] = time_resolution\n",
    "ss_ecr = ecr.SuperSelective(epsilon=epsilon,\n",
    "                                 T_list=T_list,\n",
    "                                 sigma_list=sigma_list,\n",
    "                                 amp_thresh_percentile=amp_thresh_percentile,\n",
    "                                 amp_thresh_std=amp_thresh_std,\n",
    "                                 n_corr_peaks_max=4,\n",
    "                                 raster_dur=raster_dur,\n",
    "                                 corr_type=corr_type,\n",
    "                                 adj_threshold=adj_threshold,\n",
    "                                 time_resolution=time_resolution,\n",
    "                                 use_multiprocessing=True,\n",
    "                                 verbose=False)\n",
    "\n",
    "os.makedirs(path_results, exist_ok=True)\n",
    "\n",
    "for i_file, filename in enumerate(filenames):\n",
    "    print(f'Processing file {i_file+1} of {len(filenames)}, {filename}...')\n",
    "    \n",
    "    data_to_save = {}\n",
    "    data_to_save['filename'] = filename\n",
    "    data_to_save['parameters'] = parameters\n",
    "    \n",
    "    fullname = os.path.join(path_source_files, filename)\n",
    "    ephys_file = h5py.File(fullname, 'r')\n",
    "    well_keys = list(ephys_file['recordings']['rec0000'].keys())\n",
    "\n",
    "    for i_well, well in enumerate(well_keys):\n",
    "        t_process_start = time.time()\n",
    "        print(f'Processing well {i_well+1} of {len(well_keys)}, {well}...')\n",
    "        df_spikes = pd.DataFrame(np.array(ephys_file['recordings']['rec0000'][well]['spikes']))\n",
    "        \n",
    "        # Determine spike amplitude threshold\n",
    "        thresh_spike_amp = np.percentile(df_spikes['amplitude'], thresh_spike_amp_percentile)\n",
    "        print(f'{thresh_spike_amp_percentile}% spike amplitude percentile is {thresh_spike_amp}.')       \n",
    "\n",
    "        # Filter spikes by amplitude\n",
    "        df_spikes = df_spikes[df_spikes['amplitude'] > thresh_spike_amp]\n",
    "        \n",
    "        # Get spike times\n",
    "        df_spikes['time_sec'] = df_spikes['frameno'] / fs\n",
    "        t_sig_start = np.min(df_spikes['time_sec'])\n",
    "        df_spikes['time_sec'] = df_spikes['time_sec'] - t_sig_start\n",
    "        t_sig_last = np.max(df_spikes['time_sec'])\n",
    "        print(f'Recording duration is {t_sig_last:0.4f} seconds.')\n",
    "        \n",
    "        # Aggregate spikes into separate channels\n",
    "        chan_nums = np.unique(df_spikes['channel'])\n",
    "        t_spikes_in_chan = []\n",
    "        n_spikes_in_chan = np.zeros(len(chan_nums))\n",
    "        for i_chan, chan in enumerate(chan_nums):\n",
    "            t_spikes_in_chan.append(df_spikes[df_spikes['channel']==chan]['time_sec'].values)\n",
    "            n_spikes_in_chan[i_chan] = len(t_spikes_in_chan[-1])\n",
    "\n",
    "        # Detemine threshold for spike per channel\n",
    "        thresh_spikes_per_chan = np.mean(np.log(n_spikes_in_chan)) + \\\n",
    "                                 thresh_spikes_per_chan_std * np.std(np.log(n_spikes_in_chan))\n",
    "        print(f'Channel spike count threshold is {thresh_spikes_per_chan}')\n",
    "\n",
    "        ### We'll keep all channels and filter out low-amplitude correlation peaks\n",
    "        ### in the ECR computation, rather than filter out individual channels here.\n",
    "#         # Filter out channels with too few spikes\n",
    "#         ix_keep = np.argwhere(n_spikes_in_chan >= thresh_spikes_per_chan)[:, 0]\n",
    "#         t_spikes_in_chan = [t for n, t in zip(n_spikes_in_chan, t_spikes_in_chan) if n >= thresh_spikes_per_chan]\n",
    "#         n_spikes_in_chan = n_spikes_in_chan[ix_keep]\n",
    "#         chan_nums = chan_nums[ix_keep]\n",
    "        print(f'Number of channels kept is {len(n_spikes_in_chan)}.')\n",
    "\n",
    "        \n",
    "#         ## Plot some things, if stepping through this loop with pdb debugger\n",
    "#         plt.figure(figsize=(10, 10))\n",
    "        \n",
    "#         # Spike counts per channel, sorted\n",
    "#         plt.subplot(2, 2, 1)\n",
    "#         plt.semilogy(np.sort(n_spikes_in_chan), '.')\n",
    "#         plt.xlabel('Channel rank order')\n",
    "#         plt.ylabel('Spike count')\n",
    "#         plt.title('Spike counts per by channel')\n",
    "#         plt.grid()\n",
    "        \n",
    "#         # Histogram of all spike times\n",
    "#         plt.subplot(2, 2, 2)\n",
    "#         plt.hist(df_spikes['time_sec'], bins=100)\n",
    "#         plt.grid()\n",
    "#         plt.xlabel('Spike time')\n",
    "#         plt.ylabel('Counts')\n",
    "#         plt.title('Spike time histogram')\n",
    "\n",
    "#         # Histogram of all spike amlitudes\n",
    "#         plt.subplot(2, 2, 3)\n",
    "#         plt.hist(df_spikes['amplitude'], bins=100)\n",
    "#         plt.grid()\n",
    "#         plt.xlabel('Spike amplitude')\n",
    "#         plt.ylabel('Counts')\n",
    "#         plt.title('Spike amplitude histogram')\n",
    "\n",
    "#         plt.pause(0.1)\n",
    "#         plt.draw()\n",
    "#         import pdb\n",
    "#         pdb.set_trace()\n",
    "\n",
    "\n",
    "        # Estimate Effective Connectivity Reconstruction\n",
    "        adj_matrix_predicted, votes, corr_peaks = ss_ecr(t_spikes_in_chan)\n",
    "\n",
    "\n",
    "        # Put results in a dict and save as a pickle file\n",
    "        data_to_save['results'] = {}\n",
    "        data_to_save['results']['well'] = well\n",
    "        data_to_save['results']['thresh_spike_amp'] = thresh_spike_amp\n",
    "        data_to_save['results']['thresh_spikes_per_chan'] = thresh_spikes_per_chan\n",
    "        data_to_save['results']['channel_numbers'] = chan_nums\n",
    "        data_to_save['results']['adj_matrix_predicted'] = adj_matrix_predicted\n",
    "        data_to_save['results']['votes'] = votes\n",
    "        data_to_save['results']['corr_peaks'] = corr_peaks\n",
    "\n",
    "        with open(os.path.join(path_results, filename[:-2]+'pkl'), 'wb') as f:\n",
    "            pkl.dump(data_to_save, f, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        print(f'Done. Processing took {time.time()-t_process_start:0.2f} seconds.\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oi_ecr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
