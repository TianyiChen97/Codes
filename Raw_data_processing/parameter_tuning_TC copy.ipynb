{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import traceback\n",
    "try:\n",
    "    import ruamel.yaml as yaml\n",
    "except:\n",
    "    import ruamel_yaml as yaml\n",
    "from datetime import datetime\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib as mpl\n",
    "from pprint import pprint\n",
    "from oitools import ecr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from configuration file: /cis/home/tchen94/tianyi/Organoid/Raw_data_processing/config_mea_super-selective_TC copy.yaml\n"
     ]
    }
   ],
   "source": [
    "filename_config =  \"/cis/home/tchen94/tianyi/Organoid/Raw_data_processing/config_mea_super-selective_TC copy.yaml\"\n",
    "\n",
    "print(f'Reading from configuration file: {filename_config}')\n",
    "with open(filename_config, 'r') as f:\n",
    "    yaml_loader = yaml.YAML(typ='safe', pure=True)\n",
    "    config = yaml_loader.load(f)\n",
    "\n",
    "# Path of source .h5 files\n",
    "path_source_files = config['paths']['source_files']\n",
    "if not path_source_files.endswith('/'):\n",
    "    path_source_files += '/'\n",
    "\n",
    "# Path where results will be stored\n",
    "path_results = config['paths']['results']\n",
    "if not path_results.endswith('/'):\n",
    "    path_results += '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recompute': False,\n",
       " 'adj_threshold': 0.77,\n",
       " 'raster_dur': 0.0005,\n",
       " 'corr_type': 'cc',\n",
       " 'n_corr_peaks_max': 4,\n",
       " 'epsilon': 0.003,\n",
       " 'T_list': [0.02, 0.0175, 0.016],\n",
       " 'sigma_list': [0.0004, 0.00055, 0.0007]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['super_sel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 h5 files to process.\n",
      "Total size of h5 files: 0.20 GB\n"
     ]
    }
   ],
   "source": [
    "# Get a list of files that we'll analyze (h5 files in the source directory or its subdirectories)\n",
    "filenames = glob.glob(f'{path_source_files}/**/*.h5', recursive=True)\n",
    "n_chars = len(path_source_files)\n",
    "filenames = [f[n_chars:] for f in filenames]\n",
    "filenames.sort()\n",
    "filenames.reverse()\n",
    "\n",
    "print(f'Found {len(filenames)} h5 files to process.')\n",
    "total_size_gb = sum(os.path.getsize(os.path.join(path_source_files, f)) for f in filenames) / (1024 ** 3)\n",
    "print(f'Total size of h5 files: {total_size_gb:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime_start = datetime.now().strftime(\"_%Y%m%d_%Hh%Mm\")\n",
    "filename = filenames[0]\n",
    "filename_results = os.path.join(path_results, filename[:-3]+datetime_start+'.pkl')\n",
    "filename_results\n",
    "os.path.exists('/cis/project/organoid/Dec_10_2024_ecr_results_no_window/241018/M07915/Stimulation/000295/data.raw_20241213_18h15m.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1 of 1, data.raw.h5...\n",
      "Processing well 1 of 6, well000...\n",
      "2% spike amplitude percentile is -506.1573486328125.\n",
      "Recording duration is 866.3213 seconds.\n",
      "\n",
      "Processing window 1 of 1.\n",
      "Mean spikes per second per channel is 2.62.\n",
      "\n",
      "Using maximum of 15 CPUs for processing.\n",
      "\n",
      "\n",
      "T = 0.0225, sigma = 0.0004\n",
      "Computing correlation peaks for neuron pairs... Error while processing well well000 of fullname.\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2568571/3456811015.py\", line 99, in <module>\n",
      "    adj_matrix_predicted, votes, corr_peaks = ss_ecr(t_spikes_in_chan_in_window)\n",
      "                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/cis/home/tchen94/tianyi/ephys_analysis/oitools/oitools/ecr.py\", line 273, in __call__\n",
      "    taus_amps = pool.map(self.get_corr_peaks, pool_inputs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/cis/home/tchen94/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 367, in map\n",
      "    return self._map_async(func, iterable, mapstar, chunksize).get()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/cis/home/tchen94/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 768, in get\n",
      "    self.wait(timeout)\n",
      "  File \"/cis/home/tchen94/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 765, in wait\n",
      "    self._event.wait(timeout)\n",
      "  File \"/cis/home/tchen94/anaconda3/lib/python3.11/threading.py\", line 622, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/cis/home/tchen94/anaconda3/lib/python3.11/threading.py\", line 320, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Create an object that will run the ECR super-selective algorithm using the specified config parameters\n",
    "ss_ecr = ecr.SuperSelective(epsilon=config['super_sel']['epsilon'],\n",
    "                            T_list=config['super_sel']['T_list'],\n",
    "                            sigma_list=config['super_sel']['sigma_list'],\n",
    "                            amp_thresh_percentile=config['data']['corr_amp_thresh_percentile'],\n",
    "                            amp_thresh_std=config['data']['corr_amp_thresh_std'],\n",
    "                            n_corr_peaks_max=config['super_sel']['n_corr_peaks_max'],\n",
    "                            raster_dur=config['super_sel']['raster_dur'],\n",
    "                            corr_type=config['super_sel']['corr_type'],\n",
    "                            adj_threshold=config['super_sel']['adj_threshold'],\n",
    "                            time_resolution=1/config['data']['fs'],\n",
    "                            use_multiprocessing=True,\n",
    "                            verbose=True)\n",
    "\n",
    "os.makedirs(path_results, exist_ok=True)\n",
    "datetime_start = datetime.now().strftime(\"_%Y%m%d_%Hh%Mm\")\n",
    "\n",
    "\n",
    "# Analyze data for all files and all wells (all organoids)\n",
    "for i_file, filename in enumerate(filenames):\n",
    "    print(f'Processing file {i_file+1} of {len(filenames)}, {filename}...')\n",
    "\n",
    "    filename_results = os.path.join(path_results, filename[:-3]+datetime_start+'.pkl')\n",
    "    filename_results_dir, _ = os.path.split(filename_results)\n",
    "    os.makedirs(filename_results_dir, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    if os.path.exists(filename_results) and not config['super_sel']['recompute']:\n",
    "        # A results file already exists. Don't recompute.\n",
    "        continue\n",
    "        \n",
    "    data_to_save = {}\n",
    "    data_to_save['source_filename'] = filename\n",
    "    data_to_save['config'] = config\n",
    "    \n",
    "    fullname = os.path.join(path_source_files, filename)\n",
    "    ephys_file = h5py.File(fullname, 'r')\n",
    "    well_keys = list(ephys_file['recordings']['rec0000'].keys())\n",
    "\n",
    "# Process only the first well\n",
    "    if well_keys:\n",
    "        well = well_keys[1]  \n",
    "        i_well = 1\n",
    "        try:\n",
    "            t_process_start = time.time()\n",
    "            #print(f'Processing well {i_well+1} of {len(well_keys)}, {well}...')\n",
    "            df_spikes = pd.DataFrame(np.array(ephys_file['recordings']['rec0000'][well]['spikes']))\n",
    "\n",
    "            # Confirm that the sampling rate matches what is given in the config file\n",
    "            fs_h5 = ephys_file['recordings']['rec0000'][well]['settings']['sampling'][0]\n",
    "            assert config['data']['fs']==fs_h5, 'Sampling rate of h5 file'\n",
    "\n",
    "            # Determine spike amplitude threshold\n",
    "            spike_amp_thresh = np.percentile(df_spikes['amplitude'], config['data']['spike_amp_thresh_percentile'])\n",
    "            print(f\"{config['data']['spike_amp_thresh_percentile']}% spike amplitude percentile is {spike_amp_thresh}.\")\n",
    "\n",
    "            # Filter spikes by amplitude\n",
    "            df_spikes = df_spikes[df_spikes['amplitude'] > spike_amp_thresh]\n",
    "\n",
    "            # Get spike times\n",
    "            df_spikes['time_sec'] = df_spikes['frameno'] / config['data']['fs']\n",
    "            t_sig_start = np.min(df_spikes['time_sec'])\n",
    "            df_spikes['time_sec'] = df_spikes['time_sec'] - t_sig_start\n",
    "            t_sig_last = np.max(df_spikes['time_sec'])\n",
    "            print(f'Recording duration is {t_sig_last:0.4f} seconds.')\n",
    "\n",
    "            # Aggregate spikes into separate channels\n",
    "            chan_nums = np.unique(df_spikes['channel'])\n",
    "            t_spikes_in_chan = []\n",
    "            n_spikes_in_chan = np.zeros(len(chan_nums))\n",
    "            for i_chan, chan in enumerate(chan_nums):\n",
    "                t_spikes_in_chan.append(df_spikes[df_spikes['channel']==chan]['time_sec'].values)\n",
    "                n_spikes_in_chan[i_chan] = len(t_spikes_in_chan[-1])\n",
    "\n",
    "            # Put results in a dict and save as a pickle file\n",
    "            data_to_save[well] = {}\n",
    "            data_to_save[well]['spike_amp_thresh'] = spike_amp_thresh\n",
    "            data_to_save[well]['channel_numbers'] = chan_nums\n",
    "            data_to_save[well]['channel_spikes_per_sec'] = n_spikes_in_chan / t_sig_last\n",
    "\n",
    "            ## Process ECR in overlapping windows\n",
    "            if config['windows']['win_dur']=='None':\n",
    "                t_win_start = [0]\n",
    "                t_win_stop = [t_sig_last + 0.01]\n",
    "                win_dur = t_sig_last\n",
    "            else:\n",
    "                win_dur = config['windows']['win_dur']\n",
    "                overlap_dur = config['windows']['win_overlap_dur']\n",
    "                t_win_start = np.arange(0, t_sig_last-overlap_dur, win_dur)\n",
    "                t_win_stop = t_win_start + win_dur\n",
    "\n",
    "            for i_win, (t_start, t_stop) in enumerate(zip(t_win_start, t_win_stop)):\n",
    "                print(f'\\nProcessing window {i_win+1} of {len(t_win_start)}.')\n",
    "                t_spikes_in_chan_in_window = [[t for t in chan if t>=t_start and t<t_stop] for chan in t_spikes_in_chan]\n",
    "                mean_spikes = np.mean([len(t) for t in t_spikes_in_chan_in_window])\n",
    "                print(f'Mean spikes per second per channel is {mean_spikes/win_dur:0.2f}.')\n",
    "                \n",
    "                # Estimate Effective Connectivity Reconstruction\n",
    "                adj_matrix_predicted, votes, corr_peaks = ss_ecr(t_spikes_in_chan_in_window)\n",
    "                \n",
    "                num_edges = np.sum(np.sum(adj_matrix_predicted))\n",
    "    \n",
    "                data_to_save[well][f'win_{i_win}'] = {}\n",
    "                data_to_save[well][f'win_{i_win}']['adj_matrix_predicted'] = adj_matrix_predicted\n",
    "                data_to_save[well][f'win_{i_win}']['votes'] = votes\n",
    "                data_to_save[well][f'win_{i_win}']['corr_peaks'] = corr_peaks\n",
    "\n",
    "            #with open(filename_results, 'wb') as f:\n",
    "            #    pkl.dump(data_to_save, f, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "\n",
    "            print(f\"Done. Processing this well's data took {time.time()-t_process_start:0.2f} seconds. and number of edges are {num_edges} \\n\\n\")\n",
    "        except:\n",
    "            print(f\"Error while processing well {well} of fullname.\")\n",
    "            print(traceback.format_exc())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1074)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sum(sum(data_to_save['well005']['win_0']['adj_matrix_predicted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(filename_results, 'rb') as f:\n",
    "#    data = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well: well000, Num of edges: 2836, Num of nodes: 685\n",
      "Well: well001, Num of edges: 3097, Num of nodes: 777\n",
      "Well: well002, Num of edges: 40737, Num of nodes: 1006\n",
      "Well: well003, Num of edges: 16185, Num of nodes: 999\n",
      "Well: well004, Num of edges: 10408, Num of nodes: 858\n",
      "Well: well005, Num of edges: 1074, Num of nodes: 565\n"
     ]
    }
   ],
   "source": [
    "#for well in ['well000', 'well001', 'well002',  'well003' , 'well004' , 'well005']:\n",
    "#    if well in data:\n",
    "#            #adj = filter_matrix_TC(data, well)\n",
    "#        adj = data[well]['win_0']['adj_matrix_predicted']\n",
    "#        print(f\"Well: {well}, Num of edges: {sum(sum(adj))}, Num of nodes: {adj.shape[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oi_ecr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
